---
layout: single
title: 'PCA (Principle Component Analysis)'
categories: Machine_Learning_Lab
tag: [python, sklearn, machinelearning, theory]
toc: true
author_profile: false
sidebar:
    nav: 'docs'
search: true
sidebar:
    nav: "counts"
use_math: true
---

## What is PCA?

PCA is a representative method of dimensionality reduction  and is often utilized in machine learning, data mining, statistical analysis, noise removal etc. 

In layman's terms PCA reduced multi-dimensional data into a lower dimension. The question is: how? 

For instance let's say we want to reduce the dimension of a two dimesnsional data into a single dimension. We obviously can't preserve all the features and characteristics. We must still attempt to preserve as much information as we can. A method that enables us to do this is **PCA**.



## The Intuition Behind PCA 

![image-20231226145432453](/images/2023-12-26-PCA/image-20231226145432453.png)

Let's try reducing the dimension of the above graph. 

**Step 1) We first calcualte the mean value of each axis and shift the graph so that the mean becomes the origin**

![image-20231226145706129](/images/2023-12-26-PCA/image-20231226145706129.png)

**Step 2) Then we can find the line of best fit that maximizes the sum of the squared distance between the origin and the perpendicular projection of the point on to the lines**

![image-20231226150031503](/images/2023-12-26-PCA/image-20231226150031503.png)

Think about it in terms of $a^2 = b^2 + c^2$ 
Maximizing $c^2$ will lead to a minized $b^2$

**Step 3) We set PC1 then calculate the loading score**

![image-20231226150315532](/images/2023-12-26-PCA/image-20231226150315532.png)

We can calculate the loading score by turning one of the points on the line into a unit vector. Based on this vector the loading score is the ratio of x and y needed to reach that vector. 

**Step 4) We set PC2 as the line that is perpendicular to PC1 and goes through the origin**

![image-20231226150815190](/images/2023-12-26-PCA/image-20231226150815190.png)

**Step 5) We turn PC1 and PC2 then create a scree plot ** 

![image-20231226150943184](/images/2023-12-26-PCA/image-20231226150943184.png)

We turn PC1 and PC2 so that they become the x and y axis. If we sum the square differences between the origin and the projected points then divide the sum by (n-1) we can get the variance. 

Using this variance we can calculate the ratio between PC1 and PC2 and draw a screeplot showing how much of the variance is explained by each PC. 

![image-20231226151333371](/images/2023-12-26-PCA/image-20231226151333371.png)

As 89% of the data can be explained by PC1 we can reduce the data into one dimension by using PC1. 

![image-20231226151429753](/images/2023-12-26-PCA/image-20231226151429753.png)



## The Mathematics Behind the Algorithm 

First, from linear algebra we know that a matrix can represent a transformation of a vector. 
